{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain\n",
    "!pip install -q pypdf\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q openai\n",
    "!pip install -q chromadb\n",
    "!pip install -q langchain_community\n",
    "!pip install -q streamlit_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import os\n",
    "import tempfile\n",
    "from streamlit_chat import message\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "def initialize_session_state():\n",
    "    if \"history\" not in st.session_state:\n",
    "        st.session_state[\"history\"] = []\n",
    "\n",
    "    if \"generated\" not in st.session_state:\n",
    "        st.session_state[\"generated\"] = [\n",
    "            \"Hello! Feel free to ask me any questions.\"\n",
    "        ]\n",
    "\n",
    "    if \"past\" not in st.session_state:\n",
    "        st.session_state[\"past\"] = [\"Hey! ðŸ‘‹\"]\n",
    "\n",
    "\n",
    "def conversation_chat(query, chain, history):\n",
    "    result = chain({\n",
    "        \"question\": query,\n",
    "        \"chat_history\": history\n",
    "    })\n",
    "    history.append((query, result[\"answer\"]))\n",
    "    return result[\"answer\"]\n",
    "\n",
    "\n",
    "def display_chat_history(chain):\n",
    "    reply_container = st.container()\n",
    "    container = st.container()\n",
    "\n",
    "    with container:\n",
    "        with st.form(key=\"my_form\", clear_on_submit=True):\n",
    "            user_input = st.text_input(\n",
    "                \"Question:\",\n",
    "                placeholder=\"Ask about your Documents\",\n",
    "                key=\"input\"\n",
    "            )\n",
    "            submit_button = st.form_submit_button(label=\"Send\")\n",
    "\n",
    "        if submit_button and user_input:\n",
    "            with st.spinner(\"Generating response ......\"):\n",
    "                output = conversation_chat(\n",
    "                    query=user_input,\n",
    "                    chain=chain,\n",
    "                    history=st.session_state[\"history\"]\n",
    "                )\n",
    "\n",
    "            st.session_state[\"past\"].append(user_input)\n",
    "            st.session_state[\"generated\"].append(output)\n",
    "\n",
    "    if st.session_state[\"generated\"]:\n",
    "        with reply_container:\n",
    "            for i in range(len(st.session_state[\"generated\"])):\n",
    "                message(\n",
    "                    st.session_state[\"past\"][i],\n",
    "                    is_user=True,\n",
    "                    key=str(i) + \"_user\",\n",
    "                    avatar_style=\"thumbs\"\n",
    "                )\n",
    "                message(\n",
    "                    st.session_state[\"generated\"][i],\n",
    "                    key=str(i),\n",
    "                    avatar_style=\"fun-emoji\"\n",
    "                )\n",
    "\n",
    "\n",
    "def create_conversational_chain(vector_store):\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        temperature=0.1,\n",
    "        openai_api_key=\"\"\n",
    "    )\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True\n",
    "    )\n",
    "\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        memory=memory\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "def main():\n",
    "    initialize_session_state()\n",
    "    st.title(\"RAG ChatBot Using LangChain and ChatGPT\")\n",
    "    st.sidebar.title(\"Document Processing\")\n",
    "    uploaded_files = st.sidebar.file_uploader(\n",
    "        \"Upload Files\",\n",
    "        accept_multiple_files=True\n",
    "    )\n",
    "\n",
    "    if uploaded_files:\n",
    "        text = []\n",
    "        for file in uploaded_files:\n",
    "            file_extension = os.path.splitext(file.name)[1]\n",
    "            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "                temp_file.write(file.read())\n",
    "                temp_file_path = temp_file.name\n",
    "\n",
    "            loader = None\n",
    "            if file_extension == \".pdf\":\n",
    "                loader = PyPDFLoader(temp_file_path)\n",
    "            elif file_extension == \".docx\" or file_extension == \".doc\":\n",
    "                loader = Docx2txtLoader(temp_file_path)\n",
    "            elif file_extension == \".txt\":\n",
    "                loader = TextLoader(temp_file_path)\n",
    "\n",
    "            if loader:\n",
    "                text.extend(loader.load())\n",
    "                os.remove(temp_file_path)\n",
    "\n",
    "        text_splitter = CharacterTextSplitter(\n",
    "            separator=\"\\n\",\n",
    "            chunk_size=768,\n",
    "            chunk_overlap=128,\n",
    "            length_function=len\n",
    "        )\n",
    "        text_chunks = text_splitter.split_documents(text)\n",
    "\n",
    "        embedding = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={\"device\": \"cpu\"}\n",
    "        )\n",
    "\n",
    "        vector_store = Chroma.from_documents(\n",
    "            documents=text_chunks,\n",
    "            embedding=embedding,\n",
    "            persist_directory=\"chroma_store\"\n",
    "        )\n",
    "\n",
    "        chain = create_conversational_chain(vector_store=vector_store)\n",
    "\n",
    "        display_chat_history(chain=chain)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run app.py & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://loca.lt/mytunnelpassword"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
