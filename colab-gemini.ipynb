{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Code 1"
      ],
      "metadata": {
        "id": "n-Hs6qnvMiAL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNA50skr-iLU"
      },
      "outputs": [],
      "source": [
        "!pip install langchain -q\n",
        "!pip install pypdf -q\n",
        "!pip install sentence-transformers -q\n",
        "!pip install chromadb -q\n",
        "!pip install openai -q\n",
        "!pip install langchain_community -q\n",
        "!pip install langchain_google_genai -q\n",
        "!pip install langchain_openai -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQtbTxHZ-t1H"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEQ6Sfnf-vWf"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(\"data.pdf\")\n",
        "data = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=768, chunk_overlap=128)\n",
        "texts = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrw9UYgC-2nq"
      },
      "outputs": [],
      "source": [
        "embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en\")\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=texts,\n",
        "    embedding=embedding,\n",
        "    persist_directory=\"chroma_store\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1EeN6TT_def"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=\"\")  # Replace with your Google API key at https://aistudio.google.com/app/apikey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj7YYghcEu2Y"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "while True:\n",
        "    query = input()\n",
        "    if query == \"exit\":\n",
        "        break\n",
        "    result = qa({\"query\": query})\n",
        "    print(\"Gemini Pro:\", result[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code 2"
      ],
      "metadata": {
        "id": "Cx8uDlJlMpoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf2\n",
        "!pip install chromadb\n",
        "!pip install google.generativeai\n",
        "!pip install langchain-google-genai\n",
        "!pip install langchain\n",
        "!pip install langchain_community\n",
        "!pip install jupyter"
      ],
      "metadata": {
        "id": "YLkbQlx5Ku1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "gQqVnL5zKzQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=\"\")\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=\"\")"
      ],
      "metadata": {
        "id": "vtx8f1s_K1tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"data.pdf\")\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\".\",\n",
        "    chunk_size=768,\n",
        "    chunk_overlap=128,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "pages = loader.load_and_split(text_splitter)"
      ],
      "metadata": {
        "id": "pe97EkO1K6ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb=Chroma.from_documents(pages, embeddings)"
      ],
      "metadata": {
        "id": "puvaW5P-K9ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 10})"
      ],
      "metadata": {
        "id": "a1KfX3hzLGvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "You are a helpful AI assistant. Answer based on the context provided.\n",
        "context: {context}\n",
        "input: {input}\n",
        "answer:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)"
      ],
      "metadata": {
        "id": "41GnyYV3LKLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    query = input()\n",
        "    if query == \"exit\":\n",
        "        break\n",
        "    response = retrieval_chain.invoke({\"input\": query})\n",
        "    print(\"Gemini Pro:\", response[\"answer\"])"
      ],
      "metadata": {
        "id": "GBuY_anxLOWs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}